{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Human Activity Recognition Using WiFi Signals\n",
        "\n",
        "## Overview\n",
        "Human Activity Recognition (HAR) using WiFi signals leverages the unique properties of wireless channel variations to detect different activities.\n",
        "\n",
        "## Data Format\n",
        "- **WiFi signal data** is similar to image data in structure, represented in the shape `(channels, height, width)`, but with a different interpretation:\n",
        "  - `channels` → **channel**\n",
        "  - `height` → **Time Steps**\n",
        "  - `width` → **Antenna Pairs (transmitter-receiver combinations)**\n",
        "- **Labels** represent a predefined set of classes, as is typical in classification tasks."
      ],
      "metadata": {
        "id": "0NnOoFtyHilj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Reading Data"
      ],
      "metadata": {
        "id": "1GqPqJNJIWNg"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3DwIkG1iHVPg",
        "outputId": "9117bf03-ae4b-4e44-b08f-b048c6a77bbe"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading from https://www.kaggle.com/api/v1/datasets/download/jawadkc66/q2-kaust-2025?dataset_version_number=1...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 204M/204M [00:02<00:00, 84.5MB/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting files...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Path to dataset files: /root/.cache/kagglehub/datasets/jawadkc66/q2-kaust-2025/versions/1\n"
          ]
        }
      ],
      "source": [
        "!pip install -U -q gdown\n",
        "\n",
        "import torch\n",
        "import kagglehub\n",
        "\n",
        "path = kagglehub.dataset_download(\"jawadkc66/q2-kaust-2025\")\n",
        "\n",
        "print(\"Path to dataset files:\", path)\n",
        "\n",
        "data = torch.load('/root/.cache/kagglehub/datasets/jawadkc66/q2-kaust-2025/versions/1/WiFiSensingDataset.pt')"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "gKv4rUCekmwu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Task 1: Analyze the Dataset ( Stored in `data`)\n",
        "\n",
        "1. **Determine the number of unique labels** in the dataset.  \n",
        "\n",
        "2. **Determine the shape of the input data** (number of samples and features).  \n",
        "\n",
        "3. **Find the maximum value** in the dataset.  \n",
        "\n",
        "4. **Find the minimum value** in the dataset.  "
      ],
      "metadata": {
        "id": "8vkoyStvJBLl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 1. Determine the number of unique labels in y_train\n",
        "number_of_labels = len(torch.unique(data['y_train']))\n",
        "print(f\"Number of unique labels: {number_of_labels}\")\n",
        "\n",
        "# 2. Determine the shape of the input data (X_train)\n",
        "data_shape = data['X_train'].shape\n",
        "print(f\"Shape of the input data (X_train): {data_shape} (n_samples, channels, time_steps, subcarriers)\")\n",
        "\n",
        "# 3. Find the maximum value in the dataset (X_train)\n",
        "maximum_value = torch.max(data['X_train'])\n",
        "print(f\"Maximum value in X_train: {maximum_value}\")\n",
        "\n",
        "# 4. Find the minimum value in the dataset (X_train)\n",
        "minimum_value = torch.min(data['X_train'])\n",
        "print(f\"Minimum value in X_train: {minimum_value}\")"
      ],
      "metadata": {
        "id": "7lPNBr_UK1nb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7e58f96d-5d8c-4118-abe6-4bcbc1d648ba"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of unique labels: 7\n",
            "Shape of the input data (X_train): torch.Size([2500, 1, 250, 90]) (n_samples, channels, time_steps, subcarriers)\n",
            "Maximum value in X_train: 1.0\n",
            "Minimum value in X_train: 0.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Task 2: Build and Evaluate a Neural Network\n",
        "\n",
        "1. **Design a Neural Network (Maximum 5 Layers)**  \n",
        "   Build a compact neural network with no more than 5 layers. Clearly specify the type of each layer (e.g., Dense, Conv2D) and any activation functions used.\n",
        "\n",
        "2. **Evaluate Your Model**  \n",
        "   Train your network on the provided dataset and report the evaluation metrics (e.g., accuracy, loss). Discuss the performance of your model and any challenges faced during training.\n"
      ],
      "metadata": {
        "id": "zbbW9TqJJI84"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Task 2: Build and Evaluate a Neural Network\n",
        "\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "\n",
        "# Create a simple neural network (maximum 5 layers)\n",
        "class SimpleNN(nn.Module):\n",
        "    def __init__(self, input_size, num_classes):\n",
        "        super(SimpleNN, self).__init__()\n",
        "        self.model = nn.Sequential(\n",
        "            nn.Flatten(),\n",
        "            nn.Linear(input_size, 128),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(128, 64),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(64, num_classes)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.model(x)\n",
        "\n",
        "# Prepare the dataset\n",
        "X_train = data['X_train'].reshape(data['X_train'].shape[0], -1)  # Flatten for fully connected layers\n",
        "y_train = data['y_train'].long()\n",
        "X_test = data['X_test'].reshape(data['X_test'].shape[0], -1)\n",
        "y_test = data['y_test'].long()\n",
        "\n",
        "train_dataset = TensorDataset(X_train, y_train)\n",
        "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
        "\n",
        "# Initialize the model\n",
        "input_size = X_train.shape[1]\n",
        "num_classes = number_of_labels\n",
        "model = SimpleNN(input_size, num_classes)\n",
        "\n",
        "# Define loss function and optimizer\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "# Train the model\n",
        "epochs = 10\n",
        "for epoch in range(epochs):\n",
        "    for inputs, labels in train_loader:\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(inputs)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "    print(f\"Epoch {epoch+1}/{epochs}, Loss: {loss.item():.4f}\")\n",
        "\n",
        "# Evaluate the model\n",
        "with torch.no_grad():\n",
        "    test_outputs = model(X_test)\n",
        "    predictions = torch.argmax(test_outputs, axis=1)\n",
        "    accuracy = (predictions == y_test).sum().item() / y_test.size(0)\n",
        "\n",
        "print(f\"Test Accuracy: {accuracy * 100:.2f}%\")"
      ],
      "metadata": {
        "id": "hci0PeriJIGD",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ce8dd1f9-121f-4380-b930-948fab0314bb"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10, Loss: 1.6662\n",
            "Epoch 2/10, Loss: 1.4864\n",
            "Epoch 3/10, Loss: 1.9849\n",
            "Epoch 4/10, Loss: 1.2083\n",
            "Epoch 5/10, Loss: 0.4782\n",
            "Epoch 6/10, Loss: 1.7715\n",
            "Epoch 7/10, Loss: 1.2632\n",
            "Epoch 8/10, Loss: 0.7357\n",
            "Epoch 9/10, Loss: 1.3324\n",
            "Epoch 10/10, Loss: 0.7918\n",
            "Test Accuracy: 52.60%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "MAMgZ-FXJIL9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "5PHWz6VtLLT1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "5er_pdi8w5cV"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}